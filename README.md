# Phonetic Distance

[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://img.shields.io/pypi/v/phonetic-distance.svg)](https://pypi.org/project/phonetic-distance/)
$10^4$
**Una libreria Python avanzata per il calcolo di distanze fonetiche pesate, specificamente progettata per l'analisi di forme dialettali e variazioni linguistiche.**

## ðŸ“– Panoramica

Questo progetto implementa una versione avanzata dell'algoritmo di distanza di Levenshtein che tiene conto delle caratteristiche fonologiche dei suoni. A differenza delle distanze tradizionali basate sui caratteri, questa libreria:

- **Analizza le caratteristiche fonetiche** (vocale/consonante, modo, luogo di articolazione, ecc.)
- **Gestisce i diacritici** con tokenizzazione Unicode NFD
- **Supporta varianti multiple** separate da `/` in una singola cella
- **Fornisce similaritÃ  normalizzate** tra 0 e 1
- **Ãˆ ottimizzata per forme dialettali** e trascrizioni fonetiche

### ðŸŽ¯ Caso d'uso tipico
Quando si confrontano forme dialettali come `"gatto"` vs `"gÃ tto"`, una distanza di Levenshtein tradizionale le considererebbe molto diverse. Questa libreria riconosce che la differenza Ã¨ solo un accento e assegna una similaritÃ  molto alta (0.98).

## Indice

- [Panoramica](#panoramica)
- [Installazione rapida](#installazione-rapida)
- [API Reference](#api-reference)
- [Esempi pratici](#esempi-pratici)
- [Struttura del progetto](#struttura-del-progetto)
- [Come funziona](#come-funziona)
- [Simboli conosciuti e interpretazione](#simboli-conosciuti-e-interpretazione)
- [Sistema di Pesi](#sistema-di-pesi)
- [Test](#test)
- [Analisi dei Tempi di Calcolo](#analisi-dei-tempi-di-calcolo)
- [ComplessitÃ  Computazionale](#complessit%C3%A0-computazionale)
- [Estendere la libreria](#estendere-la-libreria)
- [Contribuire](#contribuire)
- [Licenza](#licenza)

## ðŸš€ Installazione rapida

Questo progetto Ã¨ pubblicato su PyPI e funziona con Python 3.8+.

Installazione rapida da PyPI:

```bash
pip install phonetic-distance
```

Test rapido dopo l'installazione:

```bash
python3 -c "from phonetic_distance import phon_similarity_normalized; print(phon_similarity_normalized('gatto','gÃ tto'))"
# Output: 0.98
```

Oppure, per installare dalla sorgente:

```bash
git clone https://github.com/filippovicidomini/phonetic-distance.git
cd phonetic-distance
pip install .
```
 

## ðŸ“š API Reference

### Funzioni principali

#### `tokenize_segments(text: str) -> list[TokenType]`
Converte una stringa in token fonetici usando normalizzazione NFD.

```python
from phonetic_distance import tokenize_segments

# Separa base e diacritici
tokens = tokenize_segments('gÃ tto')
print(tokens)
# [('g', frozenset()), ('a', frozenset({'Ì€'})), ('t', frozenset()), ('t', frozenset()), ('o', frozenset())]
```

#### `weighted_levenshtein(seq1, seq2, diac_w=0.1) -> float`
Calcola la distanza di Levenshtein pesata tra due sequenze di token.

```python
from phonetic_distance import weighted_levenshtein, tokenize_segments

tokens1 = tokenize_segments('casa')
tokens2 = tokenize_segments('kasa')
distance = weighted_levenshtein(tokens1, tokens2)
print(f"Distanza: {distance}")
```

#### `phon_similarity_normalized(form1: str, form2: str, **kwargs) -> float`
Calcola la similaritÃ  fonologica normalizzata (0-1) tra due forme.

```python
from phonetic_distance import phon_similarity_normalized

# Confronto con diacritico
sim1 = phon_similarity_normalized('gatto', 'gÃ tto')
print(f"gatto vs gÃ tto: {sim1}")  # 0.98

# Confronto foneticamente simile
sim2 = phon_similarity_normalized('casa', 'kasa') 
print(f"casa vs kasa: {sim2}")   # Alta similaritÃ 

# Confronto molto diverso
sim3 = phon_similarity_normalized('gatto', 'mare')
print(f"gatto vs mare: {sim3}")  # Bassa similaritÃ 
```

#### `concept_similarity_normalized(cell_a: str, cell_b: str, **kwargs) -> float`
Gestisce celle con varianti multiple separate da `/` e restituisce la massima similaritÃ .

```python
from phonetic_distance import concept_similarity_normalized

# Varianti multiple
cell1 = "pane/pÃ n"
cell2 = "pan/panÃ©" 
sim = concept_similarity_normalized(cell1, cell2)
print(f"SimilaritÃ  concettuale: {sim}")
```

### Parametri opzionali

- `keep_boundaries=False`: Include token di confine parola
- `diac_w=0.1`: Peso per differenze diacritiche (0.0-1.0)

## ðŸ’¡ Esempi pratici

### Esempio base: confronto dialettale

```python
from phonetic_distance import phon_similarity_normalized

# Esempi di varianti dialettali italiane
varianti = [
    ('casa', 'kasa'),      # c/k
    ('chiesa', 'chjesa'),  # ie/je  
    ('gatto', 'gÃ tto'),    # accento
    ('bello', 'bellu'),    # o/u finale
]

for v1, v2 in varianti:
    sim = phon_similarity_normalized(v1, v2)
    print(f"{v1:8} vs {v2:8} â†’ {sim:.3f}")
```

### Esempio avanzato: gestione varianti multiple

```python
from phonetic_distance import concept_similarity_normalized

# Celle con piÃ¹ varianti (tipico nei database dialettali)
esempi = [
    ("pane/pÃ n", "pan"),
    ("chiesa/chjesa", "chiesa"),  
    ("casa/kasa", "casa"),
    ("bello/bellu/beddu", "bello")
]

for cella1, cella2 in esempi:
    sim = concept_similarity_normalized(cella1, cella2)
    print(f"{cella1:15} vs {cella2:8} â†’ {sim:.3f}")
```

### Esempio: analisi corpus dialettale

```python
from phonetic_distance import concept_similarity_normalized

def trova_varianti_simili(forme, soglia=0.8):
    """Trova coppie di forme con alta similaritÃ ."""
    risultati = []
    
    for i, forma1 in enumerate(forme):
        for forma2 in forme[i+1:]:
            sim = concept_similarity_normalized(forma1, forma2)
            if sim >= soglia:
                risultati.append((forma1, forma2, sim))
    
    return sorted(risultati, key=lambda x: x[2], reverse=True)

# Esempio con forme dialettali
corpus = [
    "casa", "kasa", "koza", 
    "gatto", "gÃ tto", "gattu",
    "chiesa", "chjesa", "ghiesa"
]

varianti = trova_varianti_simili(corpus, soglia=0.85)
for v1, v2, sim in varianti:
    print(f"{v1} â‰ˆ {v2} ({sim:.3f})")
```

## ðŸ—ï¸ Struttura del progetto

```
phonetic-distance/
â”œâ”€â”€ phonetic_distance/          # Package principale
â”‚   â”œâ”€â”€ __init__.py            # Esporta API pubblica
â”‚   â””â”€â”€ core.py                # Implementazione algoritmi
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ usage.py               # Esempi di utilizzo
â”œâ”€â”€ tests/                     # Test suite
â”‚   â”œâ”€â”€ conftest.py           
â”‚   â””â”€â”€ test_similarity.py     
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dictionary.txt         # Dizionario forme
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ update_dictionary.py   # Gestione dizionario
â”œâ”€â”€ wd.py                      # CompatibilitÃ  legacy
â”œâ”€â”€ pyproject.toml            # Configurazione package
â”œâ”€â”€ requirements.txt          # Dipendenze (vuoto)
â””â”€â”€ README.md                 # Questa documentazione
```

## ðŸ”¬ Come funziona

### 1. Tokenizzazione NFD
Le stringhe sono normalizzate in NFD (Normalized Form Decomposed), separando caratteri base da diacritici:

```python
"gÃ tto" â†’ [('g',âˆ…), ('a',{Ì€}), ('t',âˆ…), ('t',âˆ…), ('o',âˆ…)]
```

### 2. Costi basati su feature
I costi di sostituzione considerano caratteristiche fonetiche:
- **Vocali**: apertura, anterioritÃ , arrotondamento
- **Consonanti**: luogo, modo, voce

### 3. Gestione diacritici
Differenze diacritiche hanno penalitÃ  ridotte (default 0.1).

### 4. Normalizzazione
La distanza viene normalizzata per la lunghezza massima delle sequenze.

## ðŸ§¾ Simboli conosciuti e interpretazione

La libreria riconosce una serie di simboli base e multi-carattere definiti in `phonetic_distance/core.py`.

- Multi-basi (riconoscimento greedy, in NFD): `gâ€™`, `kâ€™`, `hÊ¼`, `lÊ¼`, `nÊ¼`, `rÌ¥`, `rÌ„`, `rÌƒ`, `á¹™`.

- Vocali (ogni vocale ha attributi `height` / `back` / `round`):

| Simbolo | Altezza | AnterioritÃ  | Arrotondamento |
|--------:|:--------|:------------:|:---------------:|
| a      | open    | central     | 0               |
| e      | mid     | front       | 0               |
| Ã«      | mid     | central     | 0               |
| i      | close   | front       | 0               |
| o      | mid     | back        | 1               |
| u      | close   | back        | 1               |
| Ã¶      | mid     | front       | 1               |
| Ã¼      | close   | front       | 1               |

- Semivocali (trattate come consonanti approssimanti): `iÌ¯` (palatal, `approx`, voiced), `uÌ¯` (velar, `approx`, voiced).

- Consonanti: ogni consonante Ã¨ definita con `place`, `manner` e `voice`. Alcuni esempi:

| Simbolo | Luogo       | Modo     | Voce |
|--------:|:-----------:|:--------:|:----:|
| p      | bilabial    | stop     | 0    |
| b      | bilabial    | stop     | 1    |
| t      | dental      | stop     | 0    |
| d      | dental      | stop     | 1    |
| k      | velar       | stop     | 0    |
| g      | velar       | stop     | 1    |
| s      | alveolar    | sibilant | 0    |
| Ê’      | alveolar    | affric   | 1    |
| m      | bilabial    | nasal    | 1    |
| n      | alveolar    | nasal    | 1    |
| l      | alveolar    | lateral  | 1    |
| r      | alveolar    | trill    | 1    |

- Fallback per simboli sconosciuti: se un simbolo non Ã¨ presente in `BASE_FEATS`, viene classificato come vocale se Ã¨ presente in `_VOWEL_BASES` (`a,e,i,o,u,Ã«,Ã¶,Ã¼`), altrimenti come consonante. Le sostituzioni che coinvolgono simboli sconosciuti applicano costi moderati (vedi sezione "âš–ï¸ Sistema di Pesi").

Per la lista completa dei simboli e dei relativi attributi, consulta `phonetic_distance/core.py` negli oggetti `BASE_FEATS` e `MULTI_BASES`.

## âš–ï¸ Sistema di Pesi

La libreria utilizza un sistema di pesi calibrati per riflettere la vicinanza fonologica tra suoni. Di seguito i pesi applicati nei diversi casi:

### Pesi per Diacritici

| Parametro | Valore Default | Descrizione |
|-----------|----------------|-------------|
| `diac_w` | **0.1** | Peso per ogni diacritico diverso tra due token |

**Esempio**: `gatto` vs `gÃ tto` â†’ differenza di 1 diacritico â†’ penalitÃ  = 0.1

### Pesi per Inserimento/Cancellazione

| Tipo di Token | Parametro | Costo | Descrizione |
|---------------|-----------|-------|-------------|
| Vocale | `vowel_cost` | **1.0** | Inserimento o cancellazione di una vocale |
| Consonante | `cons_cost` | **1.1** | Inserimento o cancellazione di una consonante |
| Boundary | `boundary_cost` | **0.2** | Inserimento o cancellazione di un confine parola |

**Razionale**: Le consonanti hanno un costo leggermente superiore perchÃ© generalmente piÃ¹ distintive; i boundary hanno costo basso perchÃ© meno rilevanti foneticamente.

### Pesi per Sostituzione delle Basi

#### Sostituzioni Speciali

| Caso | Costo | Descrizione |
|------|-------|-------------|
| Base identica | **0.0** | Nessuna sostituzione |
| Vocale â†” Consonante | **1.3** | Cambio di categoria fonologica |
| Simboli sconosciuti (stesso tipo) | **0.9** | Entrambi vocali o entrambi consonanti |
| Simboli sconosciuti (tipo diverso) | **1.3** | Uno vocale, uno consonante |
| Boundary â†” Boundary | **0.0** | Confini identici |
| Boundary â†” Altro | **0.2** | Sostituzione confine con altro |

#### Sostituzioni tra Vocali

Il costo si calcola sommando le differenze di caratteristiche:

| Caratteristica | Differenza | PenalitÃ  |
|----------------|------------|----------|
| **Altezza** (apertura) | Diversa | **+0.4** |
| **AnterioritÃ ** (back/front) | Diversa | **+0.4** |
| **Arrotondamento** | Diverso | **+0.2** |

**Range totale**: 0.2 - 1.2 (con limite min/max applicato)

**Esempio**:
- `a` â†’ `e`: differente in altezza (+0.4) e anterioritÃ  (+0.4) = **0.8**
- `o` â†’ `u`: differente solo in altezza (+0.4) = **0.4**
- `i` â†’ `Ã¼`: differente solo in arrotondamento (+0.2) = **0.2**

#### Sostituzioni tra Consonanti

Il costo si calcola sommando le differenze di caratteristiche:

| Caratteristica | Differenza | PenalitÃ  |
|----------------|------------|----------|
| **Voce** (voiced/unvoiced) | Diversa | **+0.2** |
| **Luogo** di articolazione | Diverso | **+0.4** |
| **Modo** di articolazione | Diverso | **+0.6** |

**Range totale**: 0.2 - 1.2 (con limite min/max applicato)

**Esempio**:
- `p` â†’ `b`: solo voce differente (+0.2) = **0.2**
- `t` â†’ `k`: solo luogo differente (+0.4) = **0.4**
- `p` â†’ `s`: luogo (+0.4) e modo (+0.6) differenti = **1.0**
- `t` â†’ `d`: solo voce differente (+0.2) = **0.2**

### Personalizzazione dei Pesi

Tutti i pesi possono essere personalizzati nelle funzioni:

```python
from phonetic_distance import weighted_levenshtein, tokenize_segments

tokens1 = tokenize_segments('casa')
tokens2 = tokenize_segments('kasa')

# Pesi personalizzati
distanza = weighted_levenshtein(
    tokens1, 
    tokens2,
    diac_w=0.2,          # Diacritici piÃ¹ pesanti
    vowel_cost=0.8,      # Vocali meno costose
    cons_cost=1.2,       # Consonanti piÃ¹ costose
    boundary_cost=0.1    # Boundary ancora meno rilevanti
)
```

### Matrice di SimilaritÃ : Esempi Pratici

| Confronto | Differenza | Distanza | SimilaritÃ  |
|-----------|------------|----------|------------|
| `gatto` vs `gatto` | Identici | 0.0 | 1.00 |
| `gatto` vs `gÃ tto` | 1 diacritico | 0.1 | 0.98 |
| `casa` vs `kasa` | câ†’k (luogo) | ~0.4 | ~0.90 |
| `pane` vs `pani` | eâ†’i (altezza+back) | ~0.8 | ~0.80 |
| `gatto` vs `mare` | Molto diversi | >3.0 | <0.40 |

## ðŸ§ª Test

Esegui i test per verificare l'installazione:

```bash
cd phonetic-distance
python -m pytest tests/ -v
```

Oppure test rapido:

```bash
python examples/usage.py
```

## â±ï¸ Analisi dei Tempi di Calcolo

Questo progetto include strumenti dedicati per misurare e analizzare i tempi di calcolo della similaritÃ  fonologica, specialmente utili per benchmark su dataset dialettali come ALM123.

### Strumenti disponibili

#### 1. **Script Principale**: `scripts/analyze_computation_times.py`

Analizza i tempi di similaritÃ  tra le features di ciascun concetto (165 cittÃ  â†’ 165 features per concetto).

**Utilizzo rapido**:
```bash
# Test con 10 concetti (~30 secondi)
python scripts/analyze_computation_times.py --concepts 10

# Analisi completa su 369 concetti (~13 minuti)
python scripts/analyze_computation_times.py

# Con opzioni personalizzate
python scripts/analyze_computation_times.py \
  --concepts 100 \
  --output results/custom_analysis.csv \
  --save-every 25
```

**Parametri**:
- `--csv PATH`: File CSV input (default: `scripts/ALM123.csv`)
- `--output PATH`: File CSV output (default: `scripts/computation_times_analysis.csv`)
- `--concepts N`: Numero di concetti (default: tutti 369)
- `--start N`: Indice iniziale (default: 0)
- `--save-every N`: Salva ogni N concetti (default: 50)

#### 2. **Script Helper**: `scripts/run_analysis.py`

Fornisce preset predefiniti per facilitare l'esecuzione:

```bash
python scripts/run_analysis.py test      # 10 concetti
python scripts/run_analysis.py small     # 100 concetti
python scripts/run_analysis.py medium    # 200 concetti
python scripts/run_analysis.py full      # 369 concetti (completo)

# Personalizzato
python scripts/run_analysis.py custom --concepts 50 --start 100
```

### Output generato

Lo script genera due file:

1. **`computation_times_analysis.csv`** â€” Dati dettagliati per ogni concetto:
   - Indice e nome concetto
   - Numero di confronti effettuati
   - Statistiche: media, mediana, min/max, percentili (p25, p75, p95)
   - Deviazione standard e tempo totale

2. **`computation_times_summary.txt`** â€” Riepilogo esecutivo con:
   - Numero di concetti elaborati
   - Tempo totale di esecuzione
   - Statistiche aggregate sui tempi medi
   - VelocitÃ  media (confronti/sec)

### Risultati tipici

**Esempio con 10 concetti**:
```
Concetti elaborati: 10
Tempo totale: 30.79 secondi

STATISTICHE:
  - VelocitÃ  media: 3,999 confronti/sec
  - Confronti totali: 123,130
  
Tempi medi per concetto:
  - Media: 0.000253s per confronto
  - Min: 0.000078s (il mare)
  - Max: 0.000347s (fuori c'Ã¨ mare)
  
Percentili:
  - P25: 0.000209s
  - P75: 0.000313s
  - P95: 0.000337s
```

**Analisi per concetto**:
```
il mare                 | 13,366 confronti | 0.000078s media
l'alto mare             | 12,720 confronti | 0.000185s media
il mare calmo           | 13,366 confronti | 0.000262s media
il mare agitato, grosso | 12,720 confronti | 0.000319s media
```

### Note sulla performance

- **Per concetto**: ~13,500 confronti $(165Ã—164)/2$
- **Tempo/confronto**: 0.0001s - 0.0005s a seconda della lunghezza
- **Salvataggio periodico**: Ogni 50 concetti (configurabile) â€” puoi interrompere con `Ctrl+C` senza perdere dati
- **Ripresa**: Usa `--start N` per continuare da un indice precedente

### ðŸ“Š Grafici e Visualizzazione 

I dati CSV generati sono pronti per essere visualizzati. Le versioni future includeranno:
- Grafici dei tempi per concetto
- Distribuzione dei tempi di calcolo
- Analisi della velocitÃ 
- Confronti di performance tra dataset

Nel frattempo, puoi analizzare i dati con Python:

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('scripts/computation_times_analysis.csv')

# Concetti piÃ¹ lenti
print(df.nlargest(10, 'mean')[['concept_name', 'mean', 'max']])

# Distribuzione dei tempi
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
df['mean'].hist(bins=30, ax=axes[0], title='Distribuzione Tempi Medi')
df['max'].hist(bins=30, ax=axes[1], title='Distribuzione Tempi Max')
plt.tight_layout()
plt.savefig('timing_analysis.png', dpi=150)
```

## ðŸ› ï¸ Estendere la libreria

### Aggiungere nuove basi fonetiche

Modifica `BASE_FEATS` in [`phonetic_distance/core.py`](phonetic_distance/core.py):

```python
BASE_FEATS = {
    'a': {'vowel', 'open', 'central'},
    'e': {'vowel', 'mid', 'front'},
    # Aggiungi qui nuove basi...
}
```

### Gestire dizionario

Usa lo script per aggiornare il dizionario:

```bash
python scripts/update_dictionary.py --add "nuova_forma"
```

## âš–ï¸ ComplessitÃ  Computazionale

Di seguito una versione rivista e piÃ¹ precisa della complessitÃ , con esempi numerici riferiti al dataset ALM usato negli script.

- Notazione
    - n: lunghezza in caratteri di una stringa
    - t: numero di token prodotti da `tokenize_segments` (t â‰¤ n)
    - L: lunghezza tipica in token di una variante (valore medio)
    - A, B: numero di varianti in due celle (per `concept_similarity_normalized`)
    - C: numero di cittÃ  (feature per concetto)
    - K: numero di concetti

- Tokenizzazione (`tokenize_segments`)
    - Tempo: O(n) (scorre la stringa una sola volta; match multibase con lista di dimensione costante)
    - Spazio: O(t)
    - Nota: `normalize_nfd` Ã¨ cached (`lru_cache`) â€” conviene pre-tokenizzare varianti ripetute.

- Operazioni elementari
    - `base_cost`, `diac_cost`, `ins_del_cost`, `sub_cost` sono O(1) (lookup + poche operazioni aritmetiche).

- Weighted Levenshtein (DP)
    - Tempo per coppia: O(t_a * t_b) dove t_a, t_b sono i token delle due forme confrontate.
    - Spazio: O(min(t_a, t_b)) nella nostra implementazione che mantiene solo due righe DP.
    - Ottimizzazioni pratiche: earlyâ€‘abandon (soglia), banded DP (se le sequenze sono simili), caching dei token.

- `phon_similarity_normalized`
    - Esegue tokenizzazione + DP â†’ complessitÃ  dominata da O(t_a * t_b).

- `concept_similarity_normalized`
    - Con A varianti in `cell_a` e B in `cell_b` il costo Ã¨
        O(sum_{i=1..A} sum_{j=1..B} t_{a,i} * t_{b,j}) piÃ¹ il costo di tokenizzazione.
    - Se si assume una lunghezza tipica L per variante, il costo Ã¨ approssimabile con O(A * B * L^2).
    - Riduzione pratica: pre-tokenizzare tutte le varianti (una tantum) riduce l'overhead di tokenizzazione; il costo rimane il numero di coppie AÃ—B moltiplicato dal costo DP per coppia.

- Confronti su intero corpus
    - Confronti pairwise naÃ¯f su M elementi: O(M^2 * L^2). Per dataset grandi ciÃ² diventa proibitivo.
    - Nel caso specifico degli script che confrontano le feature per concetto (approccio usato qui):
        - Per concetto si confrontano le C cittÃ  tra loro: numero di confronti per concetto = C * (C - 1) / 2.
        - Complessivamente, con K concetti: totale confronti â‰ˆ K * C * (C - 1) / 2.
        - Esempio numerico (dataset ALM usato negli script): C â‰ˆ 164â€“165, K = 369 â†’
            - Per concetto â‰ˆ 13.3k confronti (es. 164â†’13,366 ; 165â†’13,530)
            - Su tutti i concetti â‰ˆ 4.9Mâ€“5.0M confronti (â‰ˆ 4,992,570 usando C=165,K=369)
        - Questo numero di confronti Ã¨ ragionevole da processare in pochi minuti su hardware moderno se il tempo per confronto Ã¨ nell'ordine dei 10^-4â€“10^-3 s (come mostrato dai benchmark locali).

- Stime temporali e suggerimenti pratici
    - Se il tempo medio per confronto Ã¨ ~0.0002â€“0.0003 s, allora 5M confronti richiedono ~1,000â€“1,500 s (â‰ˆ 16â€“25 minuti). La variabilitÃ  dipende dalla lunghezza media delle varianti e dall'overhead di tokenizzazione.
    - Consigli per velocizzare:
        - Pre-tokenizzare tutte le varianti e riutilizzare i token (cache in memoria o su disco).
        - Parallelizzare per concetto (i concetti sono indipendenti: embarrassingly parallel).
        - Applicare filtri rapidi (lunghezza, nâ€‘gram overlap) prima di eseguire DP.
        - Usare banded DP o earlyâ€‘abandon quando possibile.
        - Se serve maggior velocitÃ , implementare il nucleo `weighted_levenshtein` in C/Cython/Numba.

- Riepilogo (complessitÃ  principali)
    - Tokenizzazione: O(n) tempo, O(t) spazio
    - Weighted Levenshtein (per coppia): O(t_a * t_b) tempo, O(min(t_a, t_b)) spazio
    - `phon_similarity_normalized`: O(t_a * t_b)
    - `concept_similarity_normalized`: O(A * B * L^2) (piÃ¹ costo di tokenizzazione; riducibile con caching)

Questa versione della sezione sostituisce la precedente con numeri e formule rivisti e un esempio numerico concreto per il dataset ALM.


**Glossario (acronimi e termini usati)**

- **DP**: "dynamic programming" â€” tecnica che risolve problemi suddividendoli in sottoproblemi sovrapposti e riutilizzando i risultati (memoizzazione). Nel Levenshtein pesato si usa una matrice D con ricorrenza
    D[i,j] = min(D[i-1,j] + costo_delete,
                             D[i,j-1] + costo_insert,
                             D[i-1,j-1] + costo_substitution).
    L'implementazione del pacchetto mantiene solo due righe (`prev`, `cur`) per ridurre l'uso di memoria.

- **LRU (least recently used)**: strategia di caching che elimina l'elemento meno recentemente usato. Python fornisce `functools.lru_cache` per memoizzare funzioni (es. `normalize_nfd`).

- **NFD (Normalization Form Decomposed)**: forma Unicode che scompone caratteri composti in base + segni diacritici; utile per tokenizzare e confrontare diacritici separatamente.

- **O(...) (Bigâ€‘O)**: notazione asintotica che descrive la crescita del tempo o spazio in funzione della dimensione dell'input (es. O(n), O(n*m)).

- **LSH (localityâ€‘sensitive hashing)**: tecnica per indicizzare vettori o insiemi in modo che elementi simili abbiano elevata probabilitÃ  di collisione; utile per ridurre i candidati prima del confronto costoso.

- **ANN (approximate nearest neighbors)**: metodi per trovare vicini approssimati in grandi insiemi piÃ¹ velocemente del confronto esaustivo (es. usando LSH, HNSW).

- **BKâ€‘tree (Burkhardâ€“Keller tree)**: struttura dati per metriche discrete (come distanza edit) che permette di cercare elementi entro una certa distanza in modo efficiente.

- **MinHash**: algoritmo per stimare la similaritÃ  di Jaccard tra insiemi (es. nâ€‘gram); usato con LSH per recupero approssimato di coppie simili.

- **API**: application programming interface â€” insieme di funzioni/esportazioni che il pacchetto fornisce (`tokenize_segments`, `weighted_levenshtein`, `phon_similarity_normalized`, ...).

- **Earlyâ€‘abandon**: strategia per interrompere precocemente la DP su una coppia se il costo corrente supera una soglia stabilita (utile quando interessano solo coppie con similaritÃ  â‰¥ soglia).

- **Banded DP**: limitare la DP a una banda di ampiezza k intorno alla diagonale quando si presume che le sequenze siano simili; riduce complessitÃ  a O(k * L).



## ðŸ“‹ Requisiti

- **Python**: 3.8 o superiore
- **Dipendenze**: Nessuna (solo libreria standard)
- **Sistema**: Qualsiasi OS (Windows, macOS, Linux)

## ðŸ¤ Contribuire

1. **Segnala problemi** via [GitHub Issues](https://github.com/filippovicidomini/phonetic-distance/issues)
2. **Proponi miglioramenti** con le Pull Request
3. **Aggiungi nuove feature fonetiche** seguendo il formato esistente
4. **Migliora la documentazione** e gli esempi

### CompatibilitÃ  legacy

Il modulo `wd.py` mantiene compatibilitÃ  con codice precedente:

```python
import wd  # Deprecation warning ma funziona
# Equivalente a: import phonetic_distance
```

## ðŸ“„ Licenza

Questo progetto Ã¨ rilasciato sotto [licenza MIT](LICENSE). Libero per uso commerciale e non commerciale.

## ðŸš€ Versioni future

- [ ] Supporto per feature prosodiche (tono, stress)
- [ ] Algoritmi di clustering foneticamente consapevoli  
- [ ] Export in formati standard (JSON, CSV)
- [ ] Interfaccia web per confronti rapidi
- [ ] Modelli pre-addestrati per lingue specifiche

---

**Autore**: Filippo Vicidomini  
**Versione**: 0.1.0  
**Homepage**: https://github.com/filippovicidomini/phonetic-distance
